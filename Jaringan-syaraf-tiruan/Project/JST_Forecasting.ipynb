{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ba56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. IMPORT LIBRARY\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense \n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b092e03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DEFINISIKAN PARAMETER\n",
    "FILE_PATH = 'dataset_bbri.xlsx'\n",
    "SHEET_NAME = 'Sheet1'\n",
    "TARGET_COLUMN = 'Close' # Hanya fokus pada 'Close'\n",
    "\n",
    "# Berapa hari ke belakang yang digunakan untuk memprediksi 1 hari ke depan\n",
    "LOOK_BACK = 60 # Ini akan menjadi jumlah neuron di input layer\n",
    "\n",
    "TRAIN_SPLIT_RATIO = 0.8\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff75b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. FUNGSI HELPER (Format data untuk MLP)\n",
    "def create_dataset_mlp(dataset, look_back=1):\n",
    "    \"\"\"\n",
    "    Mengubah array nilai menjadi format dataset untuk MLP.\n",
    "    Contoh:\n",
    "    dataset = [1, 2, 3, 4, 5]\n",
    "    look_back = 2\n",
    "    Maka hasilnya:\n",
    "    dataX = [[1, 2], [2, 3], [3, 4]]\n",
    "    dataY = [3, 4, 5]\n",
    "    \"\"\"\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        # Ambil sequence [i] s/d [i+look_back] sebagai fitur\n",
    "        a = dataset[i:(i + look_back), 0]\n",
    "        dataX.append(a)\n",
    "        # Ambil data [i + look_back] sebagai target\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ea5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. MUAT DAN PROSES DATA\n",
    "print(f\"Memuat data dari {FILE_PATH} (Fitur: {TARGET_COLUMN})...\")\n",
    "try:\n",
    "    df = pd.read_excel(\n",
    "        FILE_PATH,\n",
    "        sheet_name=SHEET_NAME,\n",
    "        usecols=['Date', TARGET_COLUMN], # Muat Tanggal + 1 fitur\n",
    "        parse_dates=['Date'],\n",
    "        index_col='Date',\n",
    "        engine='openpyxl'\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{FILE_PATH}' tidak ditemukan.\")\n",
    "    exit()\n",
    "except ImportError:\n",
    "    print(\"Error: Library 'openpyxl' tidak ditemukan.\")\n",
    "    exit()\n",
    "\n",
    "df = df.dropna()\n",
    "print(\"Data berhasil dimuat.\")\n",
    "\n",
    "# Ambil nilai 'Close' dan ubah menjadi numpy array\n",
    "dataset_values = df[TARGET_COLUMN].values.reshape(-1, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7baaf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. NORMALISASI DATA\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_dataset = scaler.fit_transform(dataset_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a967ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. PISAHKAN DATA TRAINING DAN TESTING\n",
    "train_size = int(len(scaled_dataset) * TRAIN_SPLIT_RATIO)\n",
    "test_size = len(scaled_dataset) - train_size\n",
    "\n",
    "train_data = scaled_dataset[0:train_size, :]\n",
    "test_data = scaled_dataset[train_size:len(scaled_dataset), :]\n",
    "\n",
    "print(f\"Ukuran data training: {len(train_data)}\")\n",
    "print(f\"Ukuran data testing: {len(test_data)}\")\n",
    "\n",
    "# Buat dataset X dan y\n",
    "X_train, y_train = create_dataset_mlp(train_data, LOOK_BACK)\n",
    "X_test, y_test = create_dataset_mlp(test_data, LOOK_BACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. BENTUK ULANG (RESHAPE) DATA\n",
    "# Untuk MLP, kita tidak memerlukan reshape 3D.\n",
    "# Input X sudah benar: (samples, features_in) -> (samples, LOOK_BACK)\n",
    "print(f\"Shape X_train (untuk MLP): {X_train.shape}\")\n",
    "print(f\"Shape X_test (untuk MLP): {X_test.shape}\")\n",
    "print(f\"Shape y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b6fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. BANGUN MODEL MLP (MULTILAYER PERCEPTRON)\n",
    "print(\"Membangun model MLP...\")\n",
    "\n",
    "model = Sequential()\n",
    "# Input layer (Dense) dengan input_dim = LOOK_BACK\n",
    "model.add(Dense(units=100, input_dim=LOOK_BACK, activation='relu'))\n",
    "# Hidden layer\n",
    "model.add(Dense(units=50, activation='relu'))\n",
    "# Output layer (1 neuron, tanpa aktivasi / linear)\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Kompilasi model. 'adam' adalah optimizer yang menerapkan backpropagation\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e18cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. LATIH MODEL\n",
    "print(f\"Mulai training model MLP (Epochs={EPOCHS}, Batch Size={BATCH_SIZE})...\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "print(\"Training selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51237ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. LAKUKAN PREDIKSI\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdd518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. KEMBALIKAN DATA KE SKALA SEMULA (INVERSE TRANSFORM)\n",
    "# (Kembali ke cara inverse univariate yang sederhana)\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "y_train_orig = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "y_test_orig = scaler.inverse_transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01c742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. HITUNG ROOT MEAN SQUARED ERROR (RMSE)\n",
    "train_rmse = math.sqrt(mean_squared_error(y_train_orig, train_predict))\n",
    "test_rmse = math.sqrt(mean_squared_error(y_test_orig, test_predict))\n",
    "\n",
    "print(f\"\\n--- HASIL EVALUASI (MODEL MLP) ---\")\n",
    "print(f\"Train RMSE: {train_rmse:.2f}\")\n",
    "print(f\"Test RMSE:  {test_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4dde66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung MAE\n",
    "train_mae = mean_absolute_error(y_train_orig, train_predict)\n",
    "test_mae = mean_absolute_error(y_test_orig, test_predict)\n",
    "\n",
    "# Hitung MAPE (gunakan numpy.mean untuk menghindari error pembagian nol)\n",
    "# Tambahkan epsilon kecil untuk menghindari pembagian dengan nol\n",
    "epsilon = 1e-10\n",
    "train_mape = np.mean(np.abs((y_train_orig - train_predict) / (y_train_orig + epsilon))) * 100\n",
    "test_mape = np.mean(np.abs((y_test_orig - test_predict) / (y_test_orig + epsilon))) * 100\n",
    "\n",
    "\n",
    "print(f\"\\n--- HASIL EVALUASI ---\")\n",
    "print(f\"Train MAE: {train_mae:.2f}\")\n",
    "print(f\"Test MAE:  {test_mae:.2f}\")\n",
    "print(f\"Train MAPE: {train_mape:.2f}%\")\n",
    "print(f\"Test MAPE:  {test_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98708b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. VISUALISASIKAN HASIL PREDIKSI\n",
    "print(\"Membuat plot hasil prediksi...\")\n",
    "\n",
    "# Siapkan data untuk plot\n",
    "# Buat array kosong seukuran data asli, isi dengan NaN\n",
    "train_predict_plot = np.empty_like(scaled_dataset)\n",
    "train_predict_plot[:, :] = np.nan\n",
    "# Isi bagian data training dengan hasil prediksi\n",
    "train_predict_plot[LOOK_BACK:len(train_predict) + LOOK_BACK, :] = train_predict\n",
    "\n",
    "# Lakukan hal yang sama untuk data testing\n",
    "test_predict_plot = np.empty_like(scaled_dataset)\n",
    "test_predict_plot[:, :] = np.nan\n",
    "test_plot_start_index = train_size + LOOK_BACK\n",
    "test_predict_plot[test_plot_start_index : test_plot_start_index + len(test_predict), :] = test_predict\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16, 8))\n",
    "# Plot data asli\n",
    "plt.plot(df.index, scaler.inverse_transform(scaled_dataset), label='Data Asli (Close)')\n",
    "# Plot prediksi training\n",
    "plt.plot(df.index, train_predict_plot, label='Prediksi Training')\n",
    "# Plot prediksi testing\n",
    "plt.plot(df.index, test_predict_plot, label='Prediksi Testing')\n",
    "\n",
    "plt.title(f'Prediksi Harga Saham (MLP) - Test RMSE: {test_rmse:.2f}')\n",
    "plt.xlabel('Tanggal')\n",
    "plt.ylabel('Harga Close')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ae62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. VISUALISASIKAN LOSS TRAINING VS VALIDASI\n",
    "print(\"Membuat plot loss model...\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Training & Validation Loss (Model MLP)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b6af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_residuals = y_train_orig.flatten() - train_predict.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a705e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Membuat plot autokorelasi residu training...\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_acf(train_residuals, alpha=0.05)\n",
    "plt.title('Autokorelasi Residu (ACF) - Model MLP')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autokorelasi')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ff51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_residuals = y_test_orig.flatten() - test_predict.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e5ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Membuat plot autokorelasi residu testing...\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_acf(test_residuals, alpha=0.05)\n",
    "plt.title('Autokorelasi Residu (ACF) - Model MLP')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autokorelasi')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
